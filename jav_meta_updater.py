#!/usr/bin/env python3
"""
JAV Metadata Updater for Plex
è‡ªåŠ¨ä¸º Plex ä¸­çš„ JAV è§†é¢‘æ·»åŠ åˆ†ç±»å’Œå…ƒæ•°æ®
"""

import re
import time
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import yaml
from plexapi.server import PlexServer
from plexapi.exceptions import NotFound
import requests
from bs4 import BeautifulSoup
import cloudscraper
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import os
import tempfile

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('jav_meta_updater.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class JAVNumberExtractor:
    """æå– JAV ç•ªå·çš„å·¥å…·ç±»"""
    
    PATTERNS = [
        # ä¼˜å…ˆåŒ¹é…æ ‡å‡†JAVæ ¼å¼ï¼ˆ2-5ä¸ªå­—æ¯+è¿å­—ç¬¦+æ•°å­—ï¼‰
        r'([A-Z]{2,5})-(\d{3,5})',  # æ ‡å‡†æ ¼å¼: ABC-123
        r'([A-Z]{2,5})[-_](\d{3,5})',  # å¸¦ä¸‹åˆ’çº¿: ABC_123
        r'([A-Z]{3,5})(\d{3,5})',  # æ— åˆ†éš”ç¬¦: ABC123
        r'([A-Z]{2,5})\.(\d{3,5})',  # å¸¦ç‚¹å·: ABC.123
        r'([A-Z]{1}[A-Z0-9]{2,4})-(\d{3,5})',  # ç‰¹æ®Šæ ¼å¼: 1PON-123
        r'(\d{6})[-_](\d{3})',  # çº¯æ•°å­—æ ¼å¼: 012345-123
        r'([A-Z]{2,5})[-_]([A-Z]\d{2,4})',  # ç‰¹æ®Šæ ¼å¼: ABC-A123
    ]
    
    @classmethod
    def extract(cls, filename: str) -> Optional[str]:
        """ä»æ–‡ä»¶åä¸­æå–ç•ªå·"""
        # è·å–å®Œæ•´è·¯å¾„ç”¨äºæœç´¢ï¼ˆè½¬æ¢ä¸ºå¤§å†™ï¼‰
        full_path = str(filename).upper()
        
        # ç­–ç•¥1: åœ¨è·¯å¾„ä¸­æŸ¥æ‰¾ç©ºæ ¼å‰çš„ç•ªå·ï¼ˆé€‚ç”¨äº "CJOD-160 title" è¿™ç§æ ¼å¼ï¼‰
        # åˆ†å‰²è·¯å¾„ï¼Œæ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        parts = re.split(r'[/\\]', full_path)  # åˆ†å‰²è·¯å¾„
        for part in parts:
            first_space_pos = part.find(' ')
            if first_space_pos > 0:
                potential_code = part[:first_space_pos].strip()
                for pattern in cls.PATTERNS:
                    match = re.match(f'^{pattern}$', potential_code)
                    if match:
                        prefix = match.group(1)
                        number = match.group(2)
                        return f"{prefix}-{number}"
        
        # ç­–ç•¥2: è·å–æœ€ç»ˆæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        filename_only = Path(filename).stem.upper()
        first_space_pos = filename_only.find(' ')
        if first_space_pos > 0:
            potential_code = filename_only[:first_space_pos].strip()
            for pattern in cls.PATTERNS:
                match = re.match(f'^{pattern}$', potential_code)
                if match:
                    prefix = match.group(1)
                    number = match.group(2)
                    return f"{prefix}-{number}"
        
        # ç­–ç•¥3: åœ¨æ•´ä¸ªè·¯å¾„ä¸­æœç´¢ä»»ä½•ç¬¦åˆæ ¼å¼çš„ç•ªå·
        for pattern in cls.PATTERNS:
            match = re.search(pattern, full_path)
            if match:
                prefix = match.group(1)
                number = match.group(2)
                return f"{prefix}-{number}"
        
        return None


class JavLibraryScraper:
    """JavLibrary çˆ¬è™«ç±»"""
    
    def __init__(self, base_url: str = "https://www.javlibrary.com", 
                 proxy: Optional[str] = None, 
                 timeout: int = 10,
                 language: str = "cn",
                 cookies: Optional[str] = None,
                 user_agent: Optional[str] = None,
                 rate_limit: float = 1.0,
                 max_retries: int = 3):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.language = language  # cn, en, ja
        self.rate_limit = rate_limit  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        self.max_retries = max_retries  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.last_request_time = 0  # ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
        self.consecutive_429_count = 0  # è¿ç»­429é”™è¯¯è®¡æ•°
        self.adaptive_delay = 0  # è‡ªé€‚åº”å»¶è¿Ÿ
        self.scraper = cloudscraper.create_scraper()
        
        if proxy:
            self.scraper.proxies = {
                'http': proxy,
                'https': proxy
            }
        
        # è®¾ç½® User-Agent
        if user_agent:
            self.user_agent = user_agent
        else:
            self.user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        
        self.headers = {
            'User-Agent': self.user_agent,
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # å¤„ç† Cookie
        if cookies:
            self.headers['Cookie'] = cookies
            # åŒæ—¶è®¾ç½®åˆ° scraper çš„ cookies
            self._parse_cookies(cookies)
    
    def _parse_cookies(self, cookie_string: str):
        """è§£æ Cookie å­—ç¬¦ä¸²å¹¶è®¾ç½®åˆ° scraper"""
        cookies = {}
        for item in cookie_string.split('; '):
            if '=' in item:
                key, value = item.split('=', 1)
                cookies[key] = value
        
        for key, value in cookies.items():
            self.scraper.cookies.set(key, value)
    
    def _rate_limited_request(self, method: str, url: str, **kwargs):
        """å¸¦æœ‰é¢‘ç‡é™åˆ¶å’Œé‡è¯•æœºåˆ¶çš„è¯·æ±‚"""
        # ç¡®ä¿è¯·æ±‚é—´éš”ï¼ˆåŒ…å«è‡ªé€‚åº”å»¶è¿Ÿï¼‰
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        total_delay = self.rate_limit + self.adaptive_delay
        
        if time_since_last < total_delay:
            sleep_time = total_delay - time_since_last
            logger.debug(f"è®¿é—®é¢‘ç‡é™åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’ (åŸºç¡€:{self.rate_limit}s + è‡ªé€‚åº”:{self.adaptive_delay}s)")
            time.sleep(sleep_time)
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                self.last_request_time = time.time()
                response = getattr(self.scraper, method.lower())(url, **kwargs)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 403:
                    logger.warning(f"è®¿é—®è¢«æ‹’ç» (403)ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")
                    if attempt < self.max_retries - 1:
                        # æŒ‡æ•°é€€é¿
                        wait_time = (2 ** attempt) * self.rate_limit
                        logger.info(f"ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•")
                        time.sleep(wait_time)
                        continue
                elif response.status_code == 429:
                    self.consecutive_429_count += 1
                    logger.warning(f"è¯·æ±‚é¢‘ç‡è¿‡å¿« (429)ï¼Œå°è¯• {attempt + 1}/{self.max_retries}ï¼Œè¿ç»­429æ¬¡æ•°: {self.consecutive_429_count}")
                    
                    # è‡ªé€‚åº”è°ƒæ•´ï¼šå¦‚æœè¿ç»­é‡åˆ°429ï¼Œå¢åŠ åŸºç¡€å»¶è¿Ÿ
                    if self.consecutive_429_count >= 3:
                        self.adaptive_delay = min(self.adaptive_delay + 1.0, 5.0)  # æœ€å¤šå¢åŠ 5ç§’
                        logger.info(f"ğŸŒ è‡ªé€‚åº”å‡é€Ÿ: å¢åŠ  {self.adaptive_delay:.1f}s å»¶è¿Ÿ")
                    
                    if attempt < self.max_retries - 1:
                        # 429é”™è¯¯ä½¿ç”¨æ¸è¿›å¼é€€é¿ï¼š5ç§’ -> 15ç§’ -> 30ç§’
                        wait_times = [5.0, 15.0, 30.0]
                        wait_time = wait_times[min(attempt, len(wait_times)-1)]
                        logger.info(f"ç­‰å¾… {wait_time:.2f} ç§’åé‡è¯•")
                        time.sleep(wait_time)
                        continue
                
                # æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®429è®¡æ•°å™¨
                if response.status_code == 200:
                    if self.consecutive_429_count > 0:
                        logger.debug(f"âœ… è¯·æ±‚æˆåŠŸï¼Œé‡ç½®429è®¡æ•°å™¨")
                    self.consecutive_429_count = 0
                    # é€æ¸å‡å°‘è‡ªé€‚åº”å»¶è¿Ÿ
                    if self.adaptive_delay > 0:
                        self.adaptive_delay = max(0, self.adaptive_delay - 0.5)
                
                return response
                
            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    wait_time = (2 ** attempt) * self.rate_limit
                    time.sleep(wait_time)
                else:
                    raise
        
        return None
    
    def search_by_code(self, code: str) -> Optional[Dict]:
        """æ ¹æ®ç•ªå·æœç´¢å½±ç‰‡ä¿¡æ¯"""
        try:
            # æ ¹æ®è¯­è¨€è®¾ç½®æ„å»ºURL
            lang_path = f"/{self.language}" if self.language != "en" else ""
            search_url = f"{self.base_url}{lang_path}/vl_searchbyid.php"
            params = {'keyword': code}
            
            response = self._rate_limited_request(
                'get',
                search_url, 
                params=params, 
                headers=self.headers,
                timeout=self.timeout
            )
            
            if not response or response.status_code != 200:
                logger.warning(f"æœç´¢ {code} å¤±è´¥: HTTP {response.status_code if response else 'None'}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è°ƒè¯•ï¼šè®°å½•å“åº”çŠ¶æ€å’ŒURL
            logger.debug(f"å“åº”çŠ¶æ€: {response.status_code}, URL: {response.url}")
            logger.debug(f"Content-Type: {response.headers.get('Content-Type', 'N/A')}")
            
            # æ£€æŸ¥æ˜¯å¦ç›´æ¥è·³è½¬åˆ°è¯¦æƒ…é¡µ
            if 'vl_searchbyid.php' not in response.url:
                return self._parse_detail_page(soup, code)
            
            # æœç´¢ç»“æœé¡µï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„é“¾æ¥
            video_links = soup.find_all('a', href=re.compile(r'\?v='))
            for link in video_links:
                if code.upper() in link.text.upper():
                    detail_url = self.base_url + lang_path + '/' + link['href']
                    return self._fetch_detail(detail_url, code)
            
            logger.warning(f"æœªæ‰¾åˆ°ç•ªå· {code} çš„ä¿¡æ¯")
            return None
            
        except Exception as e:
            logger.error(f"æœç´¢ {code} æ—¶å‡ºé”™: {e}")
            return None
    
    def _fetch_detail(self, url: str, code: str) -> Optional[Dict]:
        """è·å–è¯¦æƒ…é¡µä¿¡æ¯"""
        try:
            response = self._rate_limited_request('get', url, headers=self.headers, timeout=self.timeout)
            if not response or response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            return self._parse_detail_page(soup, code)
            
        except Exception as e:
            logger.error(f"è·å–è¯¦æƒ…é¡µ {url} å¤±è´¥: {e}")
            return None
    
    def _parse_detail_page(self, soup: BeautifulSoup, code: str) -> Dict:
        """è§£æè¯¦æƒ…é¡µ"""
        metadata = {
            'code': code,
            'title': '',
            'genres': [],
            'actors': [],
            'studio': '',
            'director': '',
            'release_date': '',
            'rating': 0,
            'cover_url': ''
        }
        
        try:
            # è°ƒè¯•ï¼šæ‰“å°é¡µé¢éƒ¨åˆ†å†…å®¹
            page_title = soup.find('title')
            if page_title:
                logger.debug(f"é¡µé¢æ ‡é¢˜: {page_title.text.strip()}")
            
            # æ ‡é¢˜ - å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            title_selectors = [
                'h3.post-title',
                'h3',
                'div.video h3',
                'div#video_title h3'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    metadata['title'] = title_elem.text.strip()
                    logger.debug(f"æ‰¾åˆ°æ ‡é¢˜: {metadata['title']}")
                    break
            
            # å°é¢å›¾ - å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            img_selectors = [
                'img#video_jacket_img',
                'img.video-jacket',
                'div#video_jacket img'
            ]
            
            for selector in img_selectors:
                img_elem = soup.select_one(selector)
                if img_elem and img_elem.get('src'):
                    src = img_elem['src']
                    if src.startswith('//'):
                        metadata['cover_url'] = 'https:' + src
                    elif src.startswith('/'):
                        metadata['cover_url'] = self.base_url + src
                    else:
                        metadata['cover_url'] = src
                    logger.debug(f"æ‰¾åˆ°å°é¢: {metadata['cover_url']}")
                    break
            
            # ç±»åˆ«/æ ‡ç­¾ - ä¸­æ–‡ç‰ˆç½‘ç«™ä¼šç›´æ¥è¿”å›ä¸­æ–‡
            genre_selectors = [
                'div#video_genres a.genre',
                'div.genre a',
                'span.genre a'
            ]
            
            for selector in genre_selectors:
                genre_links = soup.select(selector)
                if genre_links:
                    metadata['genres'] = [g.text.strip() for g in genre_links]
                    logger.debug(f"è·å–åˆ°çš„ç±»åˆ«: {metadata['genres']}")
                    break
            
            # æ¼”å‘˜
            cast_selectors = [
                'div#video_cast a[href*="vl_star.php"]',
                'div.cast a[href*="vl_star.php"]',
                'span.star a'
            ]
            
            for selector in cast_selectors:
                actor_links = soup.select(selector)
                if actor_links:
                    metadata['actors'] = [a.text.strip() for a in actor_links]
                    logger.debug(f"æ‰¾åˆ°æ¼”å‘˜: {metadata['actors']}")
                    break
            
            # åˆ¶ä½œå•†
            maker_selectors = [
                'a[href*="vl_maker.php"]',
                'div.maker a'
            ]
            
            for selector in maker_selectors:
                maker_elem = soup.select_one(selector)
                if maker_elem:
                    metadata['studio'] = maker_elem.text.strip()
                    logger.debug(f"æ‰¾åˆ°åˆ¶ä½œå•†: {metadata['studio']}")
                    break
            
            # å‘è¡Œå•†ï¼ˆå¤‡é€‰ï¼‰
            if not metadata['studio']:
                label_selectors = [
                    'a[href*="vl_label.php"]',
                    'div.label a'
                ]
                
                for selector in label_selectors:
                    label_elem = soup.select_one(selector)
                    if label_elem:
                        metadata['studio'] = label_elem.text.strip()
                        logger.debug(f"æ‰¾åˆ°å‘è¡Œå•†: {metadata['studio']}")
                        break
            
            # å‘è¡Œæ—¥æœŸ - åœ¨è¯¦æƒ…è¡¨æ ¼ä¸­æŸ¥æ‰¾
            info_selectors = [
                'div#video_info',
                'div.info',
                'table.info'
            ]
            
            for selector in info_selectors:
                info_div = soup.select_one(selector)
                if info_div:
                    # æŸ¥æ‰¾æ—¥æœŸæ ¼å¼
                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', info_div.text)
                    if date_match:
                        metadata['release_date'] = date_match.group(1)
                        logger.debug(f"æ‰¾åˆ°å‘è¡Œæ—¥æœŸ: {metadata['release_date']}")
                        break
            
            # è¯„åˆ†
            rating_selectors = [
                'span.score',
                'div.score',
                'span.rating'
            ]
            
            for selector in rating_selectors:
                rating_elem = soup.select_one(selector)
                if rating_elem:
                    try:
                        rating_text = rating_elem.text.strip('()')
                        metadata['rating'] = float(rating_text)
                        logger.debug(f"æ‰¾åˆ°è¯„åˆ†: {metadata['rating']}")
                        break
                    except:
                        pass
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¿¡æ¯ï¼Œä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
            if not any([metadata['title'], metadata['genres'], metadata['actors']]):
                logger.debug("æœªæ‰¾åˆ°ä¸»è¦ä¿¡æ¯ï¼Œé¡µé¢å¯èƒ½ç»“æ„ä¸åŒ")
                # æ‰“å°é¡µé¢çš„ä¸»è¦ div ç»“æ„
                main_divs = soup.find_all('div', id=True)[:10]
                logger.debug(f"é¡µé¢ä¸»è¦ div IDs: {[div.get('id') for div in main_divs]}")
                
                # ä¿å­˜é¡µé¢HTMLç”¨äºåˆ†æ
                try:
                    with open(f'debug_page_{code}.html', 'w', encoding='utf-8') as f:
                        f.write(str(soup.prettify()))
                    logger.debug(f"é¡µé¢HTMLå·²ä¿å­˜åˆ° debug_page_{code}.html")
                except:
                    pass
                
        except Exception as e:
            logger.error(f"è§£æé¡µé¢å¤±è´¥: {e}")
        
        return metadata


class PlexJAVUpdater:
    """Plex JAV å…ƒæ•°æ®æ›´æ–°å™¨"""
    
    def __init__(self, plex_url: str, plex_token: str, library_name: str, rules: Dict = None):
        self.plex = PlexServer(plex_url, plex_token)
        self.library = self.plex.library.section(library_name)
        self.scraper = None
        self.genre_mapping = {}
        self.collection_mapping = {}
        self.rules = rules or {}
    
    def set_scraper(self, scraper: JavLibraryScraper):
        """è®¾ç½®çˆ¬è™«å®ä¾‹"""
        self.scraper = scraper
    
    def set_mappings(self, genre_mapping: Dict, collection_mapping: Dict):
        """è®¾ç½®åˆ†ç±»æ˜ å°„"""
        self.genre_mapping = genre_mapping
        self.collection_mapping = collection_mapping
    
    def _download_cover(self, cover_url: str, video_title: str) -> Optional[str]:
        """ä¸‹è½½å°é¢å›¾ç‰‡å¹¶è¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„"""
        try:
            if not cover_url:
                return None
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = Path(tempfile.gettempdir()) / "jav_covers"
            temp_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨è§†é¢‘æ ‡é¢˜ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_title = re.sub(r'[^\w\-_\.]', '_', video_title)
            temp_file = temp_dir / f"{safe_title}.jpg"
            
            # å¦‚æœå·²ç»ä¸‹è½½è¿‡ï¼Œç›´æ¥è¿”å›
            if temp_file.exists():
                return str(temp_file)
            
            # ä¸‹è½½å›¾ç‰‡
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.javlibrary.com/'
            }
            
            response = requests.get(cover_url, headers=headers, timeout=30)
            if response.status_code == 200:
                with open(temp_file, 'wb') as f:
                    f.write(response.content)
                logger.info(f"å°é¢ä¸‹è½½æˆåŠŸ: {temp_file.name}")
                return str(temp_file)
            else:
                logger.warning(f"å°é¢ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"ä¸‹è½½å°é¢å¤±è´¥: {e}")
            return None
    
    def _set_video_poster(self, video, cover_path: str) -> bool:
        """è®¾ç½®è§†é¢‘å°é¢"""
        try:
            if not cover_path or not os.path.exists(cover_path):
                logger.warning("å°é¢æ–‡ä»¶è·¯å¾„æ— æ•ˆ")
                return False
            
            # æ–¹æ³•1: å°è¯•ç›´æ¥ç”¨ URL ä¸Šä¼ 
            if hasattr(video, 'uploadPoster'):
                video.uploadPoster(filepath=cover_path)
                logger.debug(f"ä½¿ç”¨ uploadPoster(filepath) æˆåŠŸ")
                return True
                
        except Exception as e:
            logger.debug(f"uploadPoster(filepath) å¤±è´¥: {e}")
            
            # æ–¹æ³•2: å°è¯•ç”¨ URL æ–¹å¼ä¸Šä¼ 
            try:
                # å…ˆå°è¯•ä»åŸå§‹ URL ç›´æ¥ä¸Šä¼ 
                cover_url = getattr(self, '_last_cover_url', None)
                if cover_url and hasattr(video, 'uploadPoster'):
                    video.uploadPoster(url=cover_url)
                    logger.debug(f"ä½¿ç”¨ uploadPoster(url) æˆåŠŸ")
                    return True
            except Exception as e2:
                logger.debug(f"uploadPoster(url) å¤±è´¥: {e2}")
            
            logger.error(f"æ‰€æœ‰å°é¢ä¸Šä¼ æ–¹æ³•éƒ½å¤±è´¥: æœ€åé”™è¯¯ {e}")
            return False
    
    def get_all_videos(self) -> List:
        """è·å–åº“ä¸­æ‰€æœ‰è§†é¢‘"""
        return self.library.all()
    
    def update_video_metadata(self, video, metadata: Dict) -> bool:
        """æ›´æ–°å•ä¸ªè§†é¢‘çš„å…ƒæ•°æ®"""
        try:
            # å¼€å§‹æ‰¹é‡ç¼–è¾‘
            video.batchEdits()
            
            # æ›´æ–°æ ‡é¢˜ï¼ˆå¦‚æœåŸæ ‡é¢˜åªæ˜¯æ–‡ä»¶åï¼‰
            if metadata['title'] and not video.title or video.title == Path(video.media[0].parts[0].file).stem:
                video.editTitle(metadata['title'])
            
            # æ›´æ–°ç±»åˆ«
            if metadata['genres']:
                # æ™ºèƒ½æ˜ å°„ï¼šæ£€æµ‹æ˜¯å¦éœ€è¦æ˜ å°„ï¼ˆåªåœ¨è·å–åˆ°è‹±æ–‡æ—¶æ‰æ˜ å°„ï¼‰
                genres_to_add = []
                collections_to_add = []
                
                for genre in metadata['genres']:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼ˆå¦‚æœå·²ç»æ˜¯ä¸­æ–‡å°±ä¸éœ€è¦æ˜ å°„ï¼‰
                    if any('\u4e00' <= char <= '\u9fff' for char in genre):
                        # å·²ç»æ˜¯ä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨
                        mapped_genre = genre
                    else:
                        # è‹±æ–‡ï¼Œéœ€è¦æ˜ å°„
                        mapped_genre = self.genre_mapping.get(genre, genre)
                    
                    if mapped_genre:
                        genres_to_add.append(mapped_genre)
                    
                    # ä¸å†ä½¿ç”¨åŸºäºç±»åˆ«çš„åˆé›†æ˜ å°„
                    pass
                
                # æ·»åŠ ç±»åˆ«
                if genres_to_add:
                    video.addGenre(genres_to_add)
                    logger.info(f"æ·»åŠ ç±»åˆ«: {', '.join(genres_to_add)}")
            
            # åˆ›å»ºæ–°çš„åˆé›†ï¼ˆåŸºäºç•ªå·å’Œæ¼”å‘˜ï¼‰
            collections_to_add = []
            
            # 1. ç•ªå·å‰ç¼€åˆé›†
            if metadata['code']:
                code_prefix = metadata['code'].split('-')[0] if '-' in metadata['code'] else metadata['code'][:3]
                collections_to_add.append(f"{code_prefix}ç³»åˆ—")
                logger.info(f"å‡†å¤‡æ·»åŠ åˆ°ç³»åˆ—åˆé›†: {code_prefix}ç³»åˆ—")
            
            # 2. ä¸»æ¼”å‘˜åˆé›†ï¼ˆç¬¬ä¸€ä¸ªæ¼”å‘˜ï¼‰
            if metadata.get('actors') and len(metadata['actors']) > 0:
                main_actor = metadata['actors'][0]
                collections_to_add.append(f"{main_actor}ä½œå“é›†")
                logger.info(f"å‡†å¤‡æ·»åŠ åˆ°æ¼”å‘˜åˆé›†: {main_actor}ä½œå“é›†")
            
            # 3. åˆ¶ä½œå•†åˆé›†ï¼ˆå¯é€‰ï¼‰
            if metadata.get('studio') and self.rules.get('add_studio_collection', False):
                collections_to_add.append(metadata['studio'])
                logger.info(f"å‡†å¤‡æ·»åŠ åˆ°åˆ¶ä½œå•†åˆé›†: {metadata['studio']}")
            
            # æ·»åŠ æ‰€æœ‰åˆé›†
            if collections_to_add:
                # å»é‡
                collections_to_add = list(set(collections_to_add))
                video.addCollection(collections_to_add)
                logger.info(f"âœ… æ·»åŠ åˆ°åˆé›†: {', '.join(collections_to_add)}")
            
            # æ›´æ–°å·¥ä½œå®¤
            if metadata['studio']:
                video.editStudio(metadata['studio'])
            
            # æ›´æ–°æ¼”å‘˜ï¼ˆä½¿ç”¨æ­£ç¡®çš„Plex APIæ–¹æ³•ï¼‰
            if metadata.get('actors'):
                logger.info(f"å¼€å§‹å¤„ç† {len(metadata['actors'])} ä¸ªæ¼”å‘˜")
                actors_to_add = metadata['actors'][:5]  # é™åˆ¶å‰5ä¸ªæ¼”å‘˜
                
                try:
                    # æ–¹æ³•1: ä½¿ç”¨ edit() æ–¹æ³•æ·»åŠ æ¼”å‘˜ (æ¨è)
                    edits = {}
                    for i, actor in enumerate(actors_to_add):
                        edits[f'actor[{i}].tag.tag'] = actor
                        edits[f'actor[{i}].locked'] = 1
                        edits[f'actor[{i}].tagging.text'] = ''  # è§’è‰²åä¸ºç©º
                    
                    video.edit(**edits)
                    logger.info(f"âœ… æ‰¹é‡æ·»åŠ æ¼”å‘˜æˆåŠŸ: {', '.join(actors_to_add)}")
                    
                except Exception as e1:
                    logger.debug(f"æ‰¹é‡æ·»åŠ æ¼”å‘˜å¤±è´¥: {e1}")
                    
                    try:
                        # æ–¹æ³•2: ä½¿ç”¨ _edit_tags æ–¹æ³•
                        for actor in actors_to_add:
                            video._edit_tags(tag="actor", items=[actor])
                        logger.info(f"âœ… æ¼”å‘˜æ·»åŠ æˆåŠŸ(_edit_tags): {', '.join(actors_to_add)}")
                        
                    except Exception as e2:
                        logger.debug(f"_edit_tags æ–¹æ³•å¤±è´¥: {e2}")
                        
                        # æ–¹æ³•3: é™çº§ä¸ºæ ‡ç­¾
                        for actor in actors_to_add:
                            video.addLabel(f"æ¼”å‘˜:{actor}")
                        logger.info(f"ğŸ“‹ æ¼”å‘˜ä½œä¸ºæ ‡ç­¾æ·»åŠ : {', '.join(actors_to_add)}")
                
                # æ·»åŠ æ¼”å‘˜æ±‡æ€»æ ‡ç­¾ï¼ˆæ–¹ä¾¿æœç´¢ï¼‰
                actors_tag = f"æ¼”å‘˜: {', '.join(metadata['actors'][:3])}"
                video.addLabel(actors_tag)
                logger.info(f"ğŸ“‹ æ¼”å‘˜æ±‡æ€»æ ‡ç­¾: {actors_tag}")
            else:
                logger.debug("æ²¡æœ‰æ¼”å‘˜ä¿¡æ¯")
            
            # æ·»åŠ ç•ªå·ä½œä¸ºæ ‡ç­¾
            if metadata['code']:
                video.addLabel(metadata['code'])
            
            # æ›´æ–°è¯„åˆ†
            if metadata['rating'] > 0:
                try:
                    video.editRating(metadata['rating'])
                except AttributeError:
                    # æŸäº›ç‰ˆæœ¬çš„ plexapi å¯èƒ½ä¸æ”¯æŒ editRating
                    logger.debug(f"è·³è¿‡è¯„åˆ†è®¾ç½®ï¼ˆAPI ä¸æ”¯æŒï¼‰: {metadata['rating']}")
                    pass
            
            # ä¸‹è½½å¹¶è®¾ç½®å°é¢
            if metadata.get('cover_url') and self.rules.get('download_covers', True):
                cover_url = metadata['cover_url']
                logger.info(f"å¼€å§‹å¤„ç†å°é¢: {cover_url[:50]}...")
                
                # ä¿å­˜cover_urlä¾›åç»­ä½¿ç”¨
                self._last_cover_url = cover_url
                
                # æ›´å‡†ç¡®çš„å°é¢æ£€æµ‹
                has_poster = False
                try:
                    # å…ˆåˆ·æ–°è§†é¢‘å¯¹è±¡ä»¥è·å–æœ€æ–°çŠ¶æ€
                    video.reload()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰å°é¢
                    has_poster = (
                        (hasattr(video, 'thumb') and video.thumb and 
                         video.thumb.strip() and 'upload://' in video.thumb) or
                        (hasattr(video, 'art') and video.art and 
                         video.art.strip() and 'upload://' in video.art)
                    )
                    logger.debug(f"å°é¢æ£€æµ‹ç»“æœ: {has_poster}")
                    if hasattr(video, 'thumb'):
                        logger.debug(f"å½“å‰thumb: {video.thumb}")
                        
                except Exception as e:
                    logger.debug(f"å°é¢æ£€æµ‹å¼‚å¸¸: {e}")
                    has_poster = False
                
                # å†³å®šæ˜¯å¦ä¸‹è½½å°é¢
                should_download = not has_poster or self.rules.get('overwrite_posters', False)
                
                if should_download:
                    # æ–¹æ³•1: ç›´æ¥ä»URLä¸Šä¼ ï¼ˆæ›´é«˜æ•ˆï¼‰
                    try:
                        if hasattr(video, 'uploadPoster'):
                            video.uploadPoster(url=cover_url)
                            logger.info(f"âœ… å°é¢è®¾ç½®æˆåŠŸ(ç›´æ¥URL): {video.title}")
                        else:
                            raise Exception("uploadPoster æ–¹æ³•ä¸å­˜åœ¨")
                    except Exception as e1:
                        logger.debug(f"ç›´æ¥URLä¸Šä¼ å¤±è´¥: {e1}")
                        
                        # æ–¹æ³•2: ä¸‹è½½åä¸Šä¼ 
                        cover_path = self._download_cover(cover_url, video.title)
                        if cover_path:
                            if self._set_video_poster(video, cover_path):
                                logger.info(f"âœ… å°é¢è®¾ç½®æˆåŠŸ(ä¸‹è½½å): {video.title}")
                            else:
                                logger.warning(f"âŒ å°é¢è®¾ç½®å¤±è´¥: {video.title}")
                        else:
                            logger.warning(f"âŒ å°é¢ä¸‹è½½å¤±è´¥: {video.title}")
                else:
                    logger.info(f"â­ï¸ è·³è¿‡å°é¢ï¼ˆå·²å­˜åœ¨ä¸”ä¸è¦†ç›–ï¼‰: {video.title}")
                    
            elif not metadata.get('cover_url'):
                logger.debug("æ²¡æœ‰å°é¢URL")
            elif not self.rules.get('download_covers', True):
                logger.debug("å°é¢ä¸‹è½½åŠŸèƒ½å·²ç¦ç”¨")
            
            # ä¿å­˜æ‰€æœ‰ç¼–è¾‘
            video.saveEdits()
            
            logger.info(f"æˆåŠŸæ›´æ–° {video.title} çš„å…ƒæ•°æ®")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–° {video.title} å¤±è´¥: {e}")
            return False
    
    def process_video(self, video) -> Tuple[str, bool, Optional[Dict]]:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        filename = Path(video.media[0].parts[0].file).name
        
        # æå–ç•ªå·
        jav_code = JAVNumberExtractor.extract(filename)
        if not jav_code:
            logger.warning(f"æ— æ³•ä» {filename} æå–ç•ªå·")
            return filename, False, None
        
        logger.info(f"å¤„ç†: {filename} -> ç•ªå·: {jav_code}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´ä¿¡æ¯ï¼ˆé¿å…é‡å¤è¯·æ±‚JavLibraryï¼‰
        has_genres = len(video.genres) > 0
        has_actors = hasattr(video, 'roles') and len(video.roles) > 0
        has_studio = hasattr(video, 'studio') and video.studio
        has_collections = len(video.collections) > 0
        
        # å¦‚æœå·²æœ‰åŸºæœ¬ä¿¡æ¯ï¼ˆç±»åˆ«å’Œæ¼”å‘˜/åˆ¶ä½œå•†ï¼‰ï¼Œä½†æ²¡æœ‰åˆé›†ï¼Œåªåˆ›å»ºåˆé›†
        if has_genres and (has_actors or has_studio) and not has_collections:
            logger.info(f"âš¡ å·²æœ‰å…ƒæ•°æ®ï¼Œä»…åˆ›å»ºåˆé›†: {jav_code}")
            
            # åˆ›å»ºç•ªå·å‰ç¼€åˆé›†
            code_prefix = jav_code.split('-')[0] if '-' in jav_code else jav_code[:3]
            video.addCollection(f"{code_prefix}ç³»åˆ—")
            logger.info(f"âœ… æ·»åŠ åˆ°ç³»åˆ—åˆé›†: {code_prefix}ç³»åˆ—")
            
            # å¦‚æœæœ‰æ¼”å‘˜ï¼Œåˆ›å»ºæ¼”å‘˜åˆé›†
            if has_actors:
                try:
                    main_actor = video.roles[0].tag if video.roles else None
                    if main_actor:
                        video.addCollection(f"{main_actor}ä½œå“é›†")
                        logger.info(f"âœ… æ·»åŠ åˆ°æ¼”å‘˜åˆé›†: {main_actor}ä½œå“é›†")
                except:
                    pass
            
            return filename, True, {"code": jav_code, "action": "ä»…æ›´æ–°åˆé›†"}
        
        # å¦‚æœå·²æœ‰å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…æ‹¬åˆé›†ï¼‰ï¼Œè·³è¿‡å¤„ç†
        if has_genres and has_collections:
            logger.info(f"âš¡ è·³è¿‡å·²å¤„ç†çš„è§†é¢‘: {jav_code}")
            return filename, True, {"code": jav_code, "action": "è·³è¿‡å·²å¤„ç†"}
        
        # éœ€è¦è·å–å…ƒæ•°æ®
        if not self.scraper:
            logger.error("æœªè®¾ç½®çˆ¬è™«å®ä¾‹")
            return filename, False, None
        
        metadata = self.scraper.search_by_code(jav_code)
        if not metadata:
            logger.warning(f"æœªæ‰¾åˆ° {jav_code} çš„å…ƒæ•°æ®")
            return filename, False, None
        
        # è°ƒè¯•ï¼šè¾“å‡ºè·å–åˆ°çš„å…ƒæ•°æ®
        logger.debug(f"è·å–åˆ°çš„å…ƒæ•°æ®: æ¼”å‘˜={len(metadata.get('actors', []))}ä¸ª, å°é¢={'æœ‰' if metadata.get('cover_url') else 'æ— '}")
        if metadata.get('actors'):
            logger.info(f"æ¼”å‘˜åˆ—è¡¨: {', '.join(metadata['actors'][:3])}")
        if metadata.get('cover_url'):
            logger.info(f"å°é¢URL: {metadata['cover_url'][:50]}...")
        
        # æ›´æ–° Plex
        success = self.update_video_metadata(video, metadata)
        
        return filename, success, metadata


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def main():
    parser = argparse.ArgumentParser(description='JAV Metadata Updater for Plex')
    parser.add_argument('--config', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†çš„è§†é¢‘æ•°é‡')
    parser.add_argument('--code', help='åªå¤„ç†æŒ‡å®šç•ªå·')
    parser.add_argument('--dry-run', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…æ›´æ–°')
    parser.add_argument('--threads', type=int, default=2, help='å¹¶å‘çº¿ç¨‹æ•°')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # åˆå§‹åŒ–çˆ¬è™«
    scraper = JavLibraryScraper(
        base_url=config.get('javlibrary', {}).get('base_url', 'https://www.javlibrary.com'),
        proxy=config.get('javlibrary', {}).get('proxy'),
        timeout=config.get('javlibrary', {}).get('timeout', 10),
        language=config.get('javlibrary', {}).get('language', 'cn'),  # é»˜è®¤ä½¿ç”¨ä¸­æ–‡
        cookies=config.get('javlibrary', {}).get('cookies'),
        user_agent=config.get('javlibrary', {}).get('user_agent'),
        rate_limit=config.get('javlibrary', {}).get('rate_limit', 1.0),  # è¯·æ±‚é—´éš”
        max_retries=config.get('javlibrary', {}).get('max_retries', 3)  # æœ€å¤§é‡è¯•æ¬¡æ•°
    )
    
    # åˆå§‹åŒ– Plex æ›´æ–°å™¨
    updater = PlexJAVUpdater(
        plex_url=config['plex']['url'],
        plex_token=config['plex']['token'],
        library_name=config['plex']['library'],
        rules=config.get('rules', {})
    )
    updater.set_scraper(scraper)
    updater.set_mappings(
        genre_mapping=config.get('genre_mapping', {}),
        collection_mapping=config.get('collection_mapping', {})
    )
    
    # è·å–æ‰€æœ‰è§†é¢‘
    videos = updater.get_all_videos()
    
    if args.limit:
        videos = videos[:args.limit]
    
    if args.code:
        # åªå¤„ç†ç‰¹å®šç•ªå·
        target_videos = []
        for video in videos:
            filename = Path(video.media[0].parts[0].file).name
            if args.code.upper() in filename.upper():
                target_videos.append(video)
        videos = target_videos
    
    logger.info(f"æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘å¾…å¤„ç†")
    
    if args.dry_run:
        logger.info("æµ‹è¯•æ¨¡å¼ï¼šåªè·å–å…ƒæ•°æ®ï¼Œä¸æ›´æ–° Plex")
    
    # å¤„ç†è§†é¢‘
    success_count = 0
    failed_count = 0
    results = []
    
    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        futures = {}
        
        for video in videos:
            if not args.dry_run:
                future = executor.submit(updater.process_video, video)
                futures[future] = video
            else:
                # æµ‹è¯•æ¨¡å¼ï¼šåªè·å–å…ƒæ•°æ®
                filename = Path(video.media[0].parts[0].file).name
                jav_code = JAVNumberExtractor.extract(filename)
                if jav_code:
                    future = executor.submit(scraper.search_by_code, jav_code)
                    futures[future] = (filename, jav_code)
        
        # ä½¿ç”¨è¿›åº¦æ¡
        with tqdm(total=len(futures), desc="å¤„ç†è¿›åº¦") as pbar:
            for future in as_completed(futures):
                try:
                    if not args.dry_run:
                        filename, success, metadata = future.result()
                        if success:
                            success_count += 1
                            results.append(f"âœ“ {filename}")
                        else:
                            failed_count += 1
                            results.append(f"âœ— {filename}")
                    else:
                        metadata = future.result()
                        filename, jav_code = futures[future]
                        if metadata:
                            success_count += 1
                            logger.info(f"æ‰¾åˆ° {jav_code}: {metadata['title']}, ç±»åˆ«: {', '.join(metadata['genres'])}")
                        else:
                            failed_count += 1
                            logger.warning(f"æœªæ‰¾åˆ° {jav_code} çš„ä¿¡æ¯")
                    
                except Exception as e:
                    failed_count += 1
                    logger.error(f"å¤„ç†å¤±è´¥: {e}")
                
                pbar.update(1)
                
                # åªåœ¨å®é™…è¯·æ±‚JavLibraryæ—¶æ‰å»¶è¿Ÿï¼ˆè·³è¿‡çš„è§†é¢‘ä¸éœ€è¦å»¶è¿Ÿï¼‰
                if metadata and not (isinstance(metadata, dict) and metadata.get('action') in ['ä»…æ›´æ–°åˆé›†', 'è·³è¿‡å·²å¤„ç†']):
                    sleep_time = config.get('javlibrary', {}).get('rate_limit', 3.0)
                    time.sleep(sleep_time)
    
    # è¾“å‡ºç»Ÿè®¡
    logger.info("=" * 50)
    logger.info(f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
    
    if results and not args.dry_run:
        logger.info("\nå¤„ç†ç»“æœ:")
        for result in results:
            logger.info(result)


if __name__ == "__main__":
    main()